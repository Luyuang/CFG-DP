# Kuavo_IL - 机器人模仿学习框架

## 项目背景

Kuavo_IL是一个基于模仿学习（Imitation Learning）的机器人控制框架，旨在通过观察人类示范动作，使机器人能够学习并复现相似的行为。本项目结合了两种先进的模仿学习方法：

1. **Diffusion Policy (DP)** - 基于扩散模型的策略学习方法，能够从高维观察数据（如图像）中学习复杂的控制策略
2. **LeRobot** - 一个通用的机器人学习框架，专注于实际机器人系统的部署和控制

本框架支持多种机器人任务，如物体抓取、玩具重排、杯子操作等，通过ROS（机器人操作系统）采集数据，并提供完整的数据处理、模型训练和推理部署流程。

## 1. 数据集处理流程

### 1.1 数据采集

使用[数采程序](https://www.lejuhub.com/highlydynamic/kuavobrain/-/blob/master/src/kuavo_data_pilot/docs/1.gradio%E7%BD%91%E9%A1%B5app%E4%BD%BF%E7%94%A8/%E5%BF%AB%E9%80%9F%E5%BC%80%E5%A7%8B.md)的网页端采集rosbag数据包。

**数据采集步骤：**
1. 启动ROS环境和相关传感器（相机、机器人关节编码器等）
2. 打开Gradio网页应用进行数据采集控制
3. 进行人类示范操作，同时记录视觉和机器人状态数据
4. 数据将以rosbag格式保存，包含多个ROS话题（topics）

**初始数据结构示例：**
```
PortableSSD(dataset)
├── dataset
│   └── Task2-RearangeToy
│       └── kuavo-rosbag
│           ├── task_pcd_test_2024-12-27-21-55-02.bag
│           └── task_pcd_test_2024-12-27-22-04-02.bag
```

**rosbag数据包通常包含以下话题：**
- `/camera/color/image_raw` - RGB相机图像
- `/camera/depth/image_rect_raw` - 深度相机图像
- `/joint_states` - 机器人关节状态
- `/tf` - 坐标变换信息
- `/gripper_status` - 夹爪状态信息

### 1.2 将rosbag数据包转换为zarr格式

[Zarr格式](https://zarr.readthedocs.io/)是一种用于存储多维数组的格式，特别适合大规模科学数据的存储和处理。在本项目中，我们将ROS数据转换为zarr格式，以便于后续的模型训练。

#### 步骤1: 准备配置文件

在运行转换命令前，需要根据您的rosbag数据的话题和消息类型，调整或创建一个新的配置文件。可参考`kuavo_utils/kuavo-1convert/Task5-GrabB.yaml`配置文件的格式。

**配置文件示例：**
```yaml
# Task5-GrabB.yaml
topics:
  camera:
    topic: /camera/color/image_raw
    msg_type: sensor_msgs/Image
    save_path: color_image
  depth:
    topic: /camera/depth/image_rect_raw
    msg_type: sensor_msgs/Image
    save_path: depth_image
  joint_states:
    topic: /joint_states
    msg_type: sensor_msgs/JointState
    save_path: joint_states
  gripper:
    topic: /gripper_status
    msg_type: std_msgs/Float32
    save_path: gripper
```

**配置文件关键字段说明：**
- `topic`: ROS话题名称
- `msg_type`: 消息类型
- `save_path`: 在zarr数据集中的保存路径

#### 步骤2: 执行转换命令

运行以下命令将bag数据集转换为zarr格式：

```bash
bash /kuavo_utils/kuavo-1convert/rosbag2zarr.sh -b /PortableSSD/DATASET/Task5-GrabB/kuavo-rosbag -c /kuavo_utils/kuavo-1convert/Task5-GrabB.yaml -n 102 -a
```

**参数说明：**
- `-b`：指定数据集存放目录，包含rosbag文件的文件夹路径
- `-c`：指定配置文件位置，用于定义话题和消息类型的映射
- `-n`：指定bag包的数量，用于处理多个bag文件
- `-a`：添加到现有的zarr数据集（可选参数），如果不指定则创建新的zarr数据集

**转换过程说明：**
1. 脚本会读取配置文件中定义的话题和消息类型
2. 依次处理每个rosbag文件，提取指定话题的数据
3. 将提取的数据转换为zarr格式，并按照配置文件中的`save_path`保存
4. 生成数据统计信息和可视化结果，保存在`plt-check`目录中

#### 步骤3: 检查转换结果

转换完成后，目录结构将如下所示：

```
PortableSSD(dataset)
├── dataset
│   └── Task2-RearangeToy
│       ├── kuavo-rosbag    # 原始rosbag数据
│       ├── kuavo-zarr      # 转换后的zarr格式数据
│       │   ├── color_image # RGB图像数据
│       │   ├── depth_image # 深度图像数据
│       │   ├── joint_states # 关节状态数据
│       │   └── gripper     # 夹爪状态数据
│       ├── plt-check       # 数据检查结果，包含数据统计和可视化
│       ├── raw-video       # 原始视频记录
│       └── sample-video    # 采样视频，用于快速预览
```

**验证数据质量：**
- 检查`plt-check`目录中的图表，确认数据的连续性和完整性
- 查看`sample-video`目录中的视频样本，确认视觉数据质量
- 使用`kuavo_utils/check.py`脚本可以进一步验证数据的有效性：
  ```bash
  python kuavo_utils/check.py --zarr_path /PortableSSD/DATASET/Task2-RearangeToy/kuavo-zarr
  ```

### 1.3 将rosbag数据包转换为lerobot格式

[LeRobot](https://github.com/learnables/lerobot)是一个用于机器人学习的框架，它使用特定的数据格式来组织和处理机器人数据。将数据转换为lerobot格式可以利用LeRobot框架提供的工具和模型。

**转换步骤：**

1. 确保已安装LeRobot依赖：
   ```bash
   pip install lerobot
   ```

2. 执行以下命令将数据转换为lerobot格式：
   ```bash
   python kuavo_data_pilot/src/kuavo_data_pilot/convert_kuavo_rosbag_to_lerobot.py --raw-dir /path/to/raw/data --repo-id <org>/<dataset-name>
   ```

**参数说明：**
- `--raw-dir`：指定原始数据目录路径，包含rosbag文件的文件夹
- `--repo-id`：指定数据集的组织和名称，用于在Hugging Face上发布数据集（可选）

**转换过程说明：**
1. 脚本会读取rosbag文件中的数据
2. 将数据转换为LeRobot框架所需的格式
3. 生成元数据文件，描述数据集的结构和属性
4. 如果指定了`repo-id`，可以将数据集上传到Hugging Face Hub

**转换后的数据结构：**
```
lerobot-dataset/
├── metadata.json       # 数据集元数据
├── episodes/           # 包含多个示范片段
│   ├── episode_0/      # 第一个示范片段
│   │   ├── actions.npy # 动作数据
│   │   ├── images.npy  # 图像数据
│   │   └── states.npy  # 状态数据
│   ├── episode_1/
│   └── ...
└── config.yaml         # 配置文件
```

**验证转换结果：**
可以使用LeRobot提供的工具来验证数据集：
```bash
python -c "import lerobot; lerobot.visualize_dataset('path/to/lerobot-dataset')"
```

## 2. DP模型训练

[Diffusion Policy](https://diffusion-policy.cs.columbia.edu/)是一种基于扩散模型的机器人控制策略学习方法，能够从高维观察数据中学习复杂的控制策略。

### 2.1 配置文件调整

在开始训练前，需要根据您的数据集调整配置文件。配置文件位于项目根目录下的`diffusion_policy/diffusion_policy/config/task`目录中。

#### 任务配置文件调整

修改或创建任务配置文件（例如：`KuavoGrabB_task.yaml`），需要调整以下内容：

```yaml
# KuavoGrabB_task.yaml 示例
name: kuavo_grab_b
dataset_path: /PortableSSD/DATASET/Task5-GrabB/kuavo-zarr
horizon: 16  # 预测时间步长
obs:
  modality:
    rgb_static:  # RGB相机观察
      shape: [3, 224, 224]  # 图像尺寸 [通道, 高度, 宽度]
      type: rgb
    depth_static:  # 深度相机观察（可选）
      shape: [1, 224, 224]
      type: depth
  key_modality: rgb_static  # 主要模态
action:
  shape: [7]  # 动作维度，例如：6个关节角度+1个夹爪状态
  type: joint_positions
train_dataset_kwargs:
  _target_: diffusion_policy.dataset.pusht_image_dataset_KuavoToy_task.PushTImageDataset
  zarr_path: ${task.dataset_path}
  horizon: ${task.horizon}
  pad_before: 0
  pad_after: 0
  seed: 42
  val_ratio: 0.02
```

**关键参数说明：**
- `dataset_path`: zarr数据集的路径
- `horizon`: 预测的时间步长，影响模型的预测范围
- `obs`: 观察空间配置，包括不同模态的数据（如RGB图像、深度图像等）
- `action`: 动作空间配置，定义机器人的控制维度
- `train_dataset_kwargs`: 训练数据集的参数，包括数据加载器的类型和参数

#### 训练配置文件调整

修改或创建训练配置文件（例如：`train_diffusion_unet_real_image_workspace_KuavoToy_task.yaml`），需要调整以下内容：

```yaml
# train_diffusion_unet_real_image_workspace_KuavoToy_task.yaml 示例
defaults:
  - _self_
  - task: KuavoGrabB_task.yaml  # 任务配置文件
  - override hydra/job_logging: colorlog
  - override hydra/hydra_logging: colorlog

name: diffusion_unet_real_image_workspace_KuavoToy
_target_: diffusion_policy.workspace.train_diffusion_unet_image_workspace.TrainDiffusionUnetImageWorkspace

hydra:
  run:
    dir: outputs/${name}/${now:%Y-%m-%d}/${now:%H-%M-%S}

obs_encoder_group:
  _target_: diffusion_policy.model.vision.cnn_encoder.CNNEncoder
  input_width: 224
  input_height: 224
  input_channels: 3
  normalize_input: true

noise_pred_net:
  _target_: diffusion_policy.model.diffusion.conditional_unet1d.ConditionalUnet1D
  input_dim: 7  # 动作维度
  global_cond_dim: 512  # 观察编码维度

dataloader:
  batch_size: 32
  num_workers: 8
  shuffle: true
  pin_memory: true
  persistent_workers: true

optimizer:
  _target_: torch.optim.AdamW
  lr: 1.0e-4
  weight_decay: 1.0e-6

training:
  device: cuda:0
  seed: 42
  debug: false
  max_train_steps: 500000
  max_epochs: null
  gradient_accumulate_every: 1
  use_ema: true
  ema_decay: 0.995
  log_freq: 1000
  save_freq: 10000
  val_freq: 10000
  val_batch_size: 10
  tqdm_interval_sec: 1.0
```

**关键参数说明：**
- `task`: 指定任务配置文件
- `obs_encoder_group`: 观察编码器配置，用于处理图像输入
- `noise_pred_net`: 噪声预测网络配置，是扩散模型的核心组件
- `dataloader`: 数据加载器配置，包括批量大小、工作线程数等
- `optimizer`: 优化器配置，包括学习率、权重衰减等
- `training`: 训练过程配置，包括设备、种子、最大训练步数等

### 2.2 数据集加载器调整

根据您的数据集特点，在`diffusion_policy/diffusion_policy/dataset`目录中调整或添加数据集加载器。数据集加载器负责从zarr文件中读取数据，并将其转换为模型可用的格式。

**示例数据集加载器：**

```python
# /diffusion_policy/dataset/pusht_image_dataset_KuavoToy_task.py
import numpy as np
import torch
import zarr
from diffusion_policy.dataset.base_dataset import BaseImageDataset

class PushTImageDataset(BaseImageDataset):
    def __init__(self,
            zarr_path,
            horizon=1,
            pad_before=0,
            pad_after=0,
            seed=0,
            val_ratio=0.0
        ):
        """
        Args:
            zarr_path: 数据集路径
            horizon: 预测时间步长
            pad_before: 在序列开始前填充的步数
            pad_after: 在序列结束后填充的步数
            seed: 随机种子
            val_ratio: 验证集比例
        """
        super().__init__()
        
        # 打开zarr数据集
        self.dataset = zarr.open(zarr_path, 'r')
        
        # 读取图像和动作数据
        self.images = self.dataset['color_image']
        self.actions = self.dataset['joint_states']
        
        # 计算有效的片段长度
        n_episodes = len(self.images)
        episode_lengths = []
        for i in range(n_episodes):
            episode_lengths.append(len(self.images[i]))
        
        # 创建索引映射
        indices = []
        for episode_idx, episode_length in enumerate(episode_lengths):
            start_idx = pad_before
            end_idx = episode_length - horizon - pad_after + 1
            for i in range(start_idx, end_idx):
                indices.append((episode_idx, i))
        
        # 划分训练集和验证集
        rng = np.random.RandomState(seed=seed)
        if val_ratio > 0:
            n_val = max(1, int(len(indices) * val_ratio))
            val_indices = rng.choice(len(indices), size=n_val, replace=False)
            val_mask = np.zeros(len(indices), dtype=bool)
            val_mask[val_indices] = True
            self.train_indices = [i for i, m in zip(indices, val_mask) if not m]
            self.val_indices = [i for i, m in zip(indices, val_mask) if m]
        else:
            self.train_indices = indices
            self.val_indices = []
        
        self.horizon = horizon
        self.pad_before = pad_before
        self.pad_after = pad_after
    
    def __len__(self):
        if self.mode == 'train':
            return len(self.train_indices)
        elif self.mode == 'val':
            return len(self.val_indices)
    
    def _get_item(self, idx):
        if self.mode == 'train':
            episode_idx, start_idx = self.train_indices[idx]
        elif self.mode == 'val':
            episode_idx, start_idx = self.val_indices[idx]
        
        # 获取图像和动作序列
        images = []
        for i in range(start_idx, start_idx + self.horizon):
            img = self.images[episode_idx][i]
            img = torch.from_numpy(img).float() / 255.0  # 归一化
            images.append(img)
        
        # 获取动作序列
        actions = []
        for i in range(start_idx, start_idx + self.horizon):
            act = self.actions[episode_idx][i]
            act = torch.from_numpy(act).float()
            actions.append(act)
        
        return {
            'obs': {
                'rgb_static': torch.stack(images, dim=0)
            },
            'action': torch.stack(actions, dim=0)
        }
```

**关键方法说明：**
- `__init__`: 初始化数据集，读取zarr数据，并创建训练/验证集索引
- `__len__`: 返回数据集长度
- `_get_item`: 获取指定索引的数据项，包括图像观察和动作序列

### 2.3 执行训练命令

完成上述配置后，运行以下命令开始训练：

```bash
python diffusion_policy/train.py --config-name train_diffusion_unet_real_image_workspace_KuavoToy_task
```

**训练过程说明：**
1. 系统会加载配置文件和数据集
2. 初始化模型、优化器和训练环境
3. 开始训练循环，包括：
   - 从数据集加载批次数据
   - 前向传播计算损失
   - 反向传播更新模型参数
   - 定期保存检查点和验证模型性能
4. 训练完成后，最终模型将保存在`outputs/`目录中

**训练监控：**
- 训练日志会实时显示在终端
- 可以使用TensorBoard查看训练进度：
  ```bash
  tensorboard --logdir outputs/diffusion_unet_real_image_workspace_KuavoToy
  ```

## 3. 模型推理

训练完成后，可以使用训练好的模型进行推理，验证模型性能或部署到实际机器人上。

### 3.1 使用Diffusion Policy模型推理

Diffusion Policy模型推理使用`eval_dp.py`脚本，该脚本加载训练好的模型，并在测试环境或实际机器人上执行策略。

**执行命令：**

```bash
python kuavo_utils/eval_dp.py --model_path outputs/diffusion_unet_real_image_workspace_KuavoToy/YYYY-MM-DD/HH-MM-SS/checkpoint/model_500000.pth --n_episodes 5
```

**参数说明：**
- `--model_path`: 模型检查点路径
- `--n_episodes`: 执行的测试片段数量
- `--render`: 是否渲染可视化结果（可选）
- `--real_robot`: 是否在实际机器人上执行（可选）

**推理过程说明：**
1. 脚本加载训练好的Diffusion Policy模型
2. 初始化测试环境或连接实际机器人
3. 对每个测试片段：
   - 获取当前观察（如相机图像）
   - 使用模型预测下一步动作
   - 执行预测的动作
   - 记录结果和性能指标
4. 输出总体性能评估结果

### 3.2 使用LeRobot模型推理

LeRobot模型推理使用`eval_le.py`脚本，该脚本加载使用LeRobot框架训练的模型，并执行类似的推理过程。

**执行命令：**

```bash
python kuavo_utils/eval_le.py --model_path /path/to/lerobot/model --n_episodes 5
```

**参数说明：**
- `--model_path`: LeRobot模型路径
- `--n_episodes`: 执行的测试片段数量
- `--render`: 是否渲染可视化结果（可选）
- `--real_robot`: 是否在实际机器人上执行（可选）

**推理过程说明：**
1. 脚本加载训练好的LeRobot模型
2. 初始化测试环境或连接实际机器人
3. 对每个测试片段执行策略并记录结果
4. 输出性能评估结果

## 4. 常见问题与解决方案

### 4.1 数据转换问题

- **问题**: rosbag转zarr过程中出现话题不匹配错误
  **解决方案**: 检查配置文件中的话题名称和消息类型是否与您的rosbag数据一致，可以使用`rosbag info`命令查看bag文件中的话题列表

- **问题**: 转换后的zarr数据集缺少某些时间步
  **解决方案**: 检查原始rosbag数据的完整性，确保数据采集过程中没有中断或丢失数据

- **问题**: 图像数据转换后质量较差
  **解决方案**: 检查相机设置和图像压缩参数，可能需要调整相机分辨率或减少压缩率

### 4.2 训练问题

- **问题**: 训练过程中内存不足
  **解决方案**: 减小批处理大小（batch size），或减少图像分辨率，或使用梯度累积技术

- **问题**: 训练损失不收敛
  **解决方案**: 检查学习率设置，可能需要调整学习率或使用学习率调度器；也可能是数据质量问题，检查数据预处理步骤

- **问题**: GPU显存溢出
  **解决方案**: 减小模型大小，或使用混合精度训练，或在较小的数据子集上进行训练

### 4.3 推理问题

- **问题**: 模型加载错误
  **解决方案**: 确认模型路径和格式是否正确，检查模型版本与代码版本是否匹配

- **问题**: 实际机器人执行效果与仿真不符
  **解决方案**: 检查仿真环境与实际环境的差异，可能需要进行域适应或在实际环境中收集更多数据进行微调

- **问题**: 动作执行过程中机器人出现抖动
  **解决方案**: 调整动作执行的平滑参数，或在后处理中添加轨迹平滑算法

## 5. 高级功能

### 5.1 数据增强

为提高模型泛化能力，可以在训练过程中使用数据增强技术：

```python
# 在数据集加载器中添加数据增强
def _get_item(self, idx):
    # ...获取原始数据...
    
    # 应用数据增强
    if self.mode == 'train':
        # 随机裁剪
        if random.random() > 0.5:
            images = [self._random_crop(img) for img in images]
        
        # 随机翻转
        if random.random() > 0.5:
            images = [torch.flip(img, dims=[-1]) for img in images]
        
        # 随机亮度和对比度调整
        if random.random() > 0.5:
            images = [self._adjust_brightness_contrast(img) for img in images]
    
    # ...返回处理后的数据...
```

### 5.2 模型集成

通过集成多个模型的预测结果，可以提高推理性能：

```python
# 在推理脚本中实现模型集成
models = []
for model_path in model_paths:
    model = load_model(model_path)
    models.append(model)

def ensemble_predict(obs):
    predictions = []
    for model in models:
        pred = model.predict(obs)
        predictions.append(pred)
    
    # 平均集成
    ensemble_pred = sum(predictions) / len(predictions)
    return ensemble_pred
```

### 5.3 在线微调

在实际部署中，可以实现在线微调功能，根据实际环境调整模型：

```python
# 在线微调示例
def online_finetune(model, new_data, lr=1e-5, steps=100):
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)
    
    for step in range(steps):
        batch = sample_batch(new_data)
        loss = model.compute_loss(batch)
        
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
    
    return model
```

## 6. 参考资料

- [Diffusion Policy论文](https://diffusion-policy.cs.columbia.edu/)
- [LeRobot框架文档](https://github.com/learnables/lerobot)
- [ROS (Robot Operating System)文档](https://www.ros.org/)
- [Zarr格式文档](https://zarr.readthedocs.io/)